# ──────────────────────────────────────────────
# Healthcare Assistant AI — docker-compose
# One command: docker compose up --build
# ──────────────────────────────────────────────
services:

  # ── Next.js Frontend ─────────────────────────
  web:
    build:
      context: ./apps/web
      dockerfile: Dockerfile
    ports:
      - "3000:3000"
    environment:
      # Browser-side fetch → must resolve on HOST, not Docker network
      - NEXT_PUBLIC_API_BASE_URL=http://localhost:8000
      # Enable file-watch polling for Windows / Docker volumes
      - WATCHPACK_POLLING=true
      - CHOKIDAR_USEPOLLING=true
    depends_on:
      - api
    volumes:
      # Hot-reload: mount source code (excludes node_modules via .dockerignore)
      - ./apps/web:/app
      - /app/node_modules           # anonymous volume to preserve container node_modules
      - /app/.next                  # anonymous volume to preserve .next build cache

  # ── FastAPI Backend ──────────────────────────
  api:
    build:
      context: .                    # repo root (needs rag/, data/, scripts/)
      dockerfile: apps/api/Dockerfile
    ports:
      - "8000:8000"
    environment:
      # Ollama: uses host.docker.internal by default (host-installed Ollama)
      # Override: docker compose --profile ollama up → use http://ollama:11434
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
      - OLLAMA_MODEL=qwen2.5:7b-instruct
      - RAG_INDEX_PATH=/app/rag/index/chroma
      - TESSERACT_CMD=tesseract     # in PATH inside container
      - CORS_ORIGINS=http://localhost:3000,http://127.0.0.1:3000
    volumes:
      # Mount RAG index read-only so you can rebuild it on host
      - ./rag:/app/rag:ro
      - ./data:/app/data:ro
    # Windows Docker Desktop: enable host.docker.internal
    extra_hosts:
      - "host.docker.internal:host-gateway"

  # ── Ollama (optional — use: docker compose --profile ollama up --build) ──
  ollama:
    image: ollama/ollama:latest
    profiles: ["ollama"]
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    # GPU support (uncomment if NVIDIA GPU available):
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

volumes:
  ollama_data:
